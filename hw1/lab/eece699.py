
"""
Author: Simon Zouki
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19GL7W7dgzdzjmlftxWLRVfCDVBL8V2p5
"""

import pandas as pd
import numpy as np
import sklearn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.pipeline import make_pipeline
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from IPython.display import clear_output, display

"""This is the start of our EECE 699 Assignment. After completing the pre-lab, I am now familiar with the SHAP library and ready to apply it on a real life example. I chose the IMDB Reviews Dataset because I am accustomed to this dataset having worked on it before. Also, its structure is clear and the work prior to the SHAP library should be straightforward, which will give us more time to focus on the part including the interpretability of the model.

We will start by reading the CSV, fixing the data in the format we want, and cleaning the data a little bit to potentially improve the accuracy of our model. More work could could have been done when it comes to data handling like lemmatization for example, but this isn't really necessary in our case since we can use a simple model, and doing so won't deeply affect our perfomance (after trying to implement it, and decided to discard after taking a lot of time to run).

## Preparation
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install shap
# import shap

df = pd.read_csv("/content/drive/MyDrive/EECE 699/IMDB.csv")

df.head()

class_names = df["sentiment"].unique()
class_names

"""We can see that we only have 2 possible values. We will implement a funcion to handle that accordingly. I decided to implement it manually instead of using an existing function in sklearn."""

def sentiment_to_bits(x):
  if x=="positive":
    return 1
  else:
    return 0

df["sentiment"] = df["sentiment"].apply(lambda x: sentiment_to_bits(x))

df.head()

"""We can see that we now have 1 and 0 instead of positive and negative, and that is what we wanted in order to feed it to our model. We will now use the cleaning function taken from a NLP project I had previously worked on, and use it on our model."""

def clean(df, text_field):
    df[text_field] = df[text_field].str.replace(r"\n", "")
    df[text_field] = df[text_field].str.replace(r"<br />", "")
    df[text_field] = df[text_field].str.replace(r"<br>", "")
    df[text_field] = df[text_field].str.replace(r"http\S+", "")
    df[text_field] = df[text_field].str.replace(r"http", "")
    df[text_field] = df[text_field].str.replace(r"@\S+", "")
    df[text_field] = df[text_field].str.replace(r"[^A-Za-z]+",' ')
    df[text_field] = df[text_field].str.lower()
    return df
   
df = standardize_text(df, "review")

df.head()

"""We can see that special character were removed and some other potentially bad input like < br />.  We will run a last function to check some general info about the df."""

df.info()

"""Everything seems fine here with no null values. We will proceed to the model.

## Model

We will start by splitting our data into training and testing data with a 80% split.
After doing so, we will initialize a vectorizer, and we chose the TfidfVectorizer instead of the CountVectorizer. We might decide to change based on the results. We fit it on X_train and run it on both X_train and X_test.
"""

X = df["review"].values
y = df["sentiment"].values
X_train_origin, X_test_origin, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=53)

vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(X_train_origin)
X_test = vectorizer.transform(X_test_origin)

"""We run a Logistic Regression model, and the reason for that is that I adopted the following approach: we start by the simplest model and see how it performs. If it performs well enough, we will stop and proceed. Else, we try hyperparameter optimization or a whole new model. 
We will start with a basic logistic regression with C=1, a penalty of l1 and a 'liblinear' solver.
"""

model = LogisticRegression(C=1, penalty='l1', solver='liblinear')
model.fit(X_train, y_train)

y_test = y_test.reshape(-1,1)
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f" % (accuracy, precision, recall, f1))

print(classification_report(y_test, y_pred))

"""Seeing the result, all validation metrics are scoring 87% and higher, which is well enough an NLP application of this level. I considered it enough and proceeded accordingly.

## SHAP

We define a shap explainer instance using the model and the X_train data. We also initialize the shap_values using the testing data and the explainer. We then cast them into arrays.
"""

explainer = shap.LinearExplainer(model, X_train, feature_dependence="independent")
shap_values = explainer.shap_values(X_test[:50])
X_test_array = X_test[:50].toarray()

"""The summary plot gives an overview about some word encountered in the selected testing data. We can see how bad, worst, annoying and such words are generally included in instances of low SHAP value, which shows that they have a negative impact on the review. Also, the colors of each data points show how much value it has on the input. """

shap.summary_plot(shap_values, X_test_array, feature_names=vectorizer.get_feature_names())

"""We will now start looking at specific  testing instances and visualize them.

### 1st Example

We will start by looking at a positive review.
"""

ind = 5

print("Positive" if y_test[ind] else "Negative", "Review")

"""We showcase the review in the following cell:"""

clear_output(wait=True)
X_test_origin[ind]

"""We will now plot the shap explanations for the testing instance using the force plot and the waterfall plots. Looking at the documentation, we write the code for the visualizations and look to analyze."""

shap.initjs()
shap.force_plot(
    explainer.expected_value, shap_values[ind,:], X_test_array[ind,:],
    feature_names=vectorizer.get_feature_names()
)

"""From the force plot, we can see how the words affect our output. Lots of words are affecting the shap value and the output positively like touching and enjoyed. Some other words like stupid and basically are affecting in the negative direction. However it is clear that the positive words are more and have more weight which caused this result to be positive. The results are consistent with the logic of reading the interview as we can sense the positive tone. We assert the following in the waterfall plot."""

shap.plots._waterfall.waterfall_legacy(explainer.expected_value, shap_values[ind], feature_names=vectorizer.get_feature_names())

"""### 2nd Example

We now look at a neutral review, by adopting the same procedure as above. While reading we might feel it's positive, but when it comes to words, they are distributed between positive and negative.
"""

ind = 10

print("Positive" if y_test[ind] else "Negative", "Review")

clear_output(wait=True)
X_test_origin[ind]

shap.initjs()
shap.force_plot(
    explainer.expected_value, shap_values[ind,:], X_test_array[ind,:],
    feature_names=vectorizer.get_feature_names()
)

"""We can see the effect of both end of the spectrum of positive and negative words. Overall, the output of SHAP is negative which doesn't really seem consistent with the logic of reading the review. However, it is explained and mainly because of the use of "big" negative words like awful, crappy ... which pushed the output in this directly. For more insights, we look at the waterfall plot."""

shap.plots._waterfall.waterfall_legacy(explainer.expected_value, shap_values[ind], feature_names=vectorizer.get_feature_names())

"""This seems to make more sense, even though E(f(x)) is still very low and not really significant. We can also see the effect of waful, crappy on the negative side but the also the big effect of positive words like great and liked.

### 3rd Example

We now choose a really negative review here.
"""

ind = 31

print("Positive" if y_test[ind] else "Negative", "Review")

"""The printed review:"""

clear_output(wait=True)
X_test_origin[ind]

shap.initjs()
shap.force_plot(
    explainer.expected_value, shap_values[ind,:], X_test_array[ind,:],
    feature_names=vectorizer.get_feature_names()
)

"""We can read the review and see how negative it is and we can see the force plot with f(x)=-3.5 which is a relatively big value (absolute value) and a negative one. This is showcased by tge use of words like worst, nothing, crap, garbage which are awfully negative and explains the solid choice of the model. We will assert this observation with the waterfall plot."""

shap.plots._waterfall.waterfall_legacy(explainer.expected_value, shap_values[ind], feature_names=vectorizer.get_feature_names())

"""Again, we can see the consistency here of how negative the review is. More significant features are to the negative side with worst, wooden (which doesn't really seem to make sense), nothing and crap leading the way. Positive words like love are scarce and don't really affect the prediction.

### 4th Example

We will now a work on a personal example. I wrote an example on my favorite movie which I rewatched last week, which is Inception. Being my favorite, my review is obviously positive.
"""

personal_positive_review_string = "I watched Inception last week, the least that could be said is that the movie was amazing. First, the actors were terrific of the likes of Leonardo Di Caprio, Ellen Page, Cillian Murphy amongst other. Also the plot was really good and is sasfying to watch, especially when all makes sense at the end of the movie. The only downside would be that the last few scenes have a few complicated dreams that are hard to understand. Amazing!"

"""We need to transform the data in a format understandable by the model like we did for the training and testing data. We will use the vectorizer that is already trained on the bulk of our data which is the training data."""

personal_positive_review = vectorizer.transform([personal_positive_review_string])

shap_values = explainer.shap_values(personal_positive_review)
X_test_array = personal_positive_review.toarray()

clear_output(wait=True)
personal_positive_review_string

shap.initjs()
shap.force_plot(
    explainer.expected_value, shap_values, X_test_array,
    feature_names=vectorizer.get_feature_names()
)

"""As expected, the force plot shows how positive the review is with the highlight on terrific and amazing, having a big effect on the output. The negative words are minimal like least which doesn't really have an effect. This is asserted in the waterfall model."""

shap.plots._waterfall.waterfall_legacy(explainer.expected_value, shap_values[0], feature_names=vectorizer.get_feature_names())

"""### 5th Example

We will do the same for a negative review with White House Down, since I am not really a fan of (intense) action movies.
"""

personal_negative_review_string = "Last week, I watched White House Down, which was a disaster to watch. Nothing happening was logical and all what I saw couldn't ever happen and is impossible. The actors were more like stunt artrists and no art was shown. Horrifying ending and overall a really bad movie."

personal_negative_review = vectorizer.transform([personal_negative_review_string])

shap_values = explainer.shap_values(personal_negative_review)
X_test_array = personal_negative_review.toarray()

clear_output(wait=True)
personal_negative_review_string

shap.initjs()
shap.force_plot(
    explainer.expected_value, shap_values, X_test_array,
    feature_names=vectorizer.get_feature_names()
)

"""A big negative value for a very negative review. Words like bad, nothing and disaster lead the way for most affecting features. The same is seen for the waterfall plot. One observation here is that there are some words like was, and, no are considered to be positive or negative which doesn't seem to make sense. One solution would be removing stopwords since they are randomly affecting the model."""

shap.plots._waterfall.waterfall_legacy(explainer.expected_value, shap_values[0], feature_names=vectorizer.get_feature_names())

"""## Saving the model and vectorizer"""

import pickle
pickle.dump(model, open("model.pkl", "wb"))
pickle.dump(vectorizer, open("vectorizer.pkl", "wb"))